
<!-- ![llm-kv-cache](https://github.com/Zefan-Cai/Awesome-LLM-KV-Cache/assets/31974251/4d9ab775-f200-471d-a289-e2b14296b633) -->

<!-- <div align='center'>
  <img src=https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg >
  <img src=https://img.shields.io/github/downloads/Zefan-Cai/Awesome-LLM-KV-Cache/total?color=ccf&label=downloads&logo=github&logoColor=lightgrey >
  <img src=https://img.shields.io/github/forks/Zefan-Cai/Awesome-LLM-KV-Cache.svg?style=social >
  <img src=https://img.shields.io/github/stars/Zefan-Cai/Awesome-LLM-KV-Cache.svg?style=social >
  <img src=https://img.shields.io/github/watchers/Zefan-Cai/Awesome-LLM-KV-Cache.svg?style=social >
  <img src=https://img.shields.io/badge/Release-v1.6-brightgreen.svg >
  <img src=https://img.shields.io/badge/License-GPLv3.0-turquoise.svg >
 </div>    -->

## ğŸ“’Introduction
Cognitive Memory in Large Language Models: A Curated List of [ğŸ“™ Papers on Cognitive Memory Mechanisms in LLMs with Codes](#paperlist). This repository is for personal use of learning and classifying the latest research papers related to cognitive memory in large language models!

## Â©ï¸Citations 


## ğŸ“–Contents 
<div id="paperlist"></div>

* ğŸ“–[Text-based Memory](#Text-based-Memory)ğŸ”¥ğŸ”¥ğŸ”¥
* ğŸ“–[KV Cache-Based Memory](#KV-Cache-Based-Memory)ğŸ”¥ğŸ”¥
* ğŸ“–[Parameters-Based Memory](#Parameters-Based-Memory)ğŸ”¥ğŸ”¥
* ğŸ“–[Hidden-State-Based Memory](#Hidden-State-Based-Memory)ğŸ”¥


<!-- ### ğŸ“–Trending Inference Topics ([Â©ï¸backğŸ‘†ğŸ»](#paperlist))  
<div id="Trending-Inference-Topics"></div>  

|Date|Title|Paper|Code|Recom|Comment|
|:---:|:---:|:---:|:---:|:---:|:---:| -->




<!-- ### Cognitive Architecture
|2020| ğŸ”¥ğŸ”¥ğŸ”¥[Kotseruba and Tsotsos] 40 years of cognitive architectures: core cognitive abilities and practical applications(@Kotseruba)|[[pdf]](https://arxiv.org/pdf/2004.08441 ) | |â­ï¸â­ï¸â­ï¸ |
|2019| ğŸ”¥ğŸ”¥ğŸ”¥[Laird] The Soar Cognitive Architecture(@Laird)|[[pdf]](https://mitpress.mit.edu/books/soar-cognitive-architecture ) | |â­ï¸â­ï¸â­ï¸ |
|2019| ğŸ”¥ğŸ”¥ğŸ”¥[Ritter et al.] The ACT-R Cognitive Architecture(@Ritter)|[[pdf]](https://act-r.psy.cmu.edu/) | |â­ï¸â­ï¸â­ï¸ |
|2006| ğŸ”¥ğŸ”¥ğŸ”¥[Langley and Choi] A unified cognitive architecture for physical agents(@Langley)|[[pdf]](https://www.aaai.org/Papers/AAAI/2006/AAAI06-1469.pdf ) | |â­ï¸â­ï¸â­ï¸ | -->

### Text-based Memory

<div id="Text-based-Memory"></div>

|Date|Title|Paper|Code|Recom|Comment|
|:---:|:---:|:---:|:---:|:---:|:---:|
|2024b| ğŸ”¥ğŸ”¥ğŸ”¥[Chen et al.] Compress to impress: Unleashing the potential of compressive memory in real-world long-term conversations(@Chen)|[[pdf]](https://arxiv.org/pdf/2402.11975 ) | |â­ï¸â­ï¸â­ï¸ |
|2024b| ğŸ”¥ğŸ”¥ğŸ”¥[Chen et al.] Compress to impress: Unleashing the potential of compressive memory in real-world long-term conversations(@Chen)|[[pdf]](https://arxiv.org/pdf/2402.11975 ) | |â­ï¸â­ï¸â­ï¸ |
|2024| ğŸ”¥ğŸ”¥ğŸ”¥[Zhong et al.] Memorybank: Enhancing large language models with long-term memory(@Zhong)|[[pdf]](https://arxiv.org/pdf/2404.02441 ) | |â­ï¸â­ï¸â­ï¸ |
|2024| ğŸ”¥ğŸ”¥ğŸ”¥[Lee et al.] A human-inspired reading agent with gist memory of very long contexts(@Lee)|[[pdf]](https://arxiv.org/pdf/2402.09727 ) | |â­ï¸â­ï¸â­ï¸ |
|2023| ğŸ”¥ğŸ”¥ğŸ”¥[Liu et al.] Think-in-memory: Recalling and post-thinking enable llms with long-term memory(@Liu)|[[pdf]](https://arxiv.org/pdf/2311.08719 ) | |â­ï¸â­ï¸â­ï¸ |
|2022| ğŸ”¥ğŸ”¥ğŸ”¥[Qi et al.] Training-free long-context scaling of large language models(@Qi)|[[pdf]](https://arxiv.org/pdf/2402.17463 ) | |â­ï¸â­ï¸â­ï¸ |
|2023| ğŸ”¥ğŸ”¥ğŸ”¥[Anagnostidis et al.] Dynamic context pruning for efficient and interpretable autoregressive transformers(@Anagnostidis)|[[pdf]](https://arxiv.org/pdf/2306.00057 ) | |â­ï¸â­ï¸â­ï¸ |
|2023| ğŸ”¥ğŸ”¥ğŸ”¥[Pagliardini et al.] Faster causal attention over large sequences through sparse flash attention(@Pagliardini)|[[pdf]](https://arxiv.org/pdf/2306.01160 ) | |â­ï¸â­ï¸â­ï¸ |
|2023| ğŸ”¥ğŸ”¥ğŸ”¥[Chevalier et al.] Adapting language models to compress contexts(@Chevalier)|[[pdf]](https://arxiv.org/pdf/2305.14788 ) | |â­ï¸â­ï¸â­ï¸ |
|2024| ğŸ”¥ğŸ”¥ğŸ”¥[Corallo and Papotti] Finch: Prompt-guided key-value cache compression for large language models(@Corallo)|[[pdf]](https://arxiv.org/pdf/2404.18057 ) | |â­ï¸â­ï¸â­ï¸ |
|2023| ğŸ”¥ğŸ”¥ğŸ”¥[Bertsch et al.] Unlimiformer: Long-range transformers with unlimited length input(@Bertsch)|[[pdf]](https://arxiv.org/pdf/2312.17259 ) | |â­ï¸â­ï¸â­ï¸ |
|2023| ğŸ”¥ğŸ”¥ğŸ”¥[Yu et al.] Mamba: A multiscale decoder architecture for long contexts(@Yu)|[[pdf]](https://arxiv.org/pdf/2312.08618 ) | |â­ï¸â­ï¸â­ï¸ |
|2023| ğŸ”¥ğŸ”¥ğŸ”¥[Xie et al.] Chunk, align, select: A simple long-sequence processing method for transformers(@Xie)|[[pdf]](https://arxiv.org/pdf/2308.13191 ) | |â­ï¸â­ï¸â­ï¸ |
|2024| ğŸ”¥ğŸ”¥ğŸ”¥[Ye et al.] Chunkattention: Efficient self-attention with prefix-aware kv cache and two-phase partition(@Ye)|[[pdf]](https://arxiv.org/pdf/2402.15220 ) | |â­ï¸â­ï¸â­ï¸ |
|2024| ğŸ”¥ğŸ”¥ğŸ”¥[An et al.] Training-free long-context scaling of large language models(@An)|[[pdf]](https://arxiv.org/pdf/2402.17463 ) | |â­ï¸â­ï¸â­ï¸ |
|2023| ğŸ”¥ğŸ”¥ğŸ”¥[Ivgi et al.] SLED: Sliding Encoder and Decoder(@Ivgi)|[[pdf]](https://arxiv.org/pdf/2311.18743 ) | |â­ï¸â­ï¸â­ï¸ |

### KV Cache-Based Memory
<div id="KV-Cache-Based-Memory"></div>

|Date|Title|Paper|Code|Recom|Comment|
|:---:|:---:|:---:|:---:|:---:|:---:|
|2024| ğŸ”¥ğŸ”¥ğŸ”¥[Wan et al.] D2o: Dynamic discriminative operations for efficient generative inference of large language models(@Wan)|[[pdf]](https://arxiv.org/pdf/2406.13035 ) | |â­ï¸â­ï¸â­ï¸ |
|2022| ğŸ”¥ğŸ”¥ğŸ”¥[Guo et al.] Longt5: Efficient text-to-text transformer for long sequences(@Guo)|[[pdf]](https://arxiv.org/pdf/2204.08441 ) | |â­ï¸â­ï¸â­ï¸ |
|2024b| ğŸ”¥ğŸ”¥ğŸ”¥[Liu et al.] Think-in-memory: Recalling and post-thinking enable llms with long-term memory(@Liu)|[[pdf]](https://arxiv.org/pdf/2311.08719 ) | |â­ï¸â­ï¸â­ï¸ |
|2024| ğŸ”¥ğŸ”¥ğŸ”¥[Dai et al.] Sequence can secretly tell you what to discard(@Dai)|[[pdf]](https://arxiv.org/pdf/2404.02222 ) | |â­ï¸â­ï¸â­ï¸ |
|2024| ğŸ”¥ğŸ”¥ğŸ”¥[Xiao et al.] Snapkv: Llm knows what you are looking for before generation(@Xiao)|[[pdf]](https://arxiv.org/pdf/2404.18057 ) | |â­ï¸â­ï¸â­ï¸ |
|2023| ğŸ”¥ğŸ”¥ğŸ”¥[Ding et al.] Longnet: Scaling transformers to 1,000,000,000 tokens(@Ding)|[[pdf]](https://arxiv.org/pdf/2307.02486 ) | |â­ï¸â­ï¸â­ï¸ |
|2020| ğŸ”¥ğŸ”¥ğŸ”¥[Beltagy et al.] Longformer: The long-document transformer(@Beltagy)|[[pdf]](https://arxiv.org/pdf/2004.05150 ) | |â­ï¸â­ï¸â­ï¸ |
|2020| ğŸ”¥ğŸ”¥ğŸ”¥[Ainslie et al.] Etc: Encoding long and structured inputs in transformers(@Ainslie)|[[pdf]](https://arxiv.org/pdf/2004.08441 ) | |â­ï¸â­ï¸â­ï¸ |
|2023| ğŸ”¥ğŸ”¥ğŸ”¥[Gao et al.] Empower your model with longer and better context comprehension(@Gao)|[[pdf]](https://arxiv.org/pdf/2307.13365 ) | |â­ï¸â­ï¸â­ï¸ |
|2024| ğŸ”¥ğŸ”¥ğŸ”¥[Jo and Shin] A2sf: Accumulative attention scoring with forgetting factor for token pruning in transformer decoder(@Jo)|[[pdf]](https://arxiv.org/pdf/2407.20485 ) | |â­ï¸â­ï¸â­ï¸ |
|2024b| ğŸ”¥ğŸ”¥ğŸ”¥[Tang et al.] Quest: Query-aware sparsity for efficient long-context llm inference(@Tang)|[[pdf]](https://arxiv.org/pdf/2406.10774 ) | |â­ï¸â­ï¸â­ï¸ |
|2024| ğŸ”¥ğŸ”¥ğŸ”¥[Zhang et al.] Efficient sparse attention needs adaptive token release(@Zhang)|[[pdf]](https://arxiv.org/pdf/2407.02328 ) | |â­ï¸â­ï¸â­ï¸ |
|2024| ğŸ”¥ğŸ”¥ğŸ”¥[Adnan et al.] Keyformer: Kv cache reduction through key tokens selection for efficient generative inference(@Adnan)|[[pdf]](https://arxiv.org/pdf/2404.12335 ) | |â­ï¸â­ï¸â­ï¸ |
|2024| ğŸ”¥ğŸ”¥ğŸ”¥[Zhao et al.] Alisa: Accelerating large language model inference via sparsity-aware kv caching(@Zhao)|[[pdf]](https://arxiv.org/pdf/2407.01143 ) | |â­ï¸â­ï¸â­ï¸ |
|2024| ğŸ”¥ğŸ”¥ğŸ”¥[Kim et al.] Compressed context memory for online language model interaction(@Kim)|[[pdf]](https://arxiv.org/pdf/2312.03414 ) | |â­ï¸â­ï¸â­ï¸ |
|2024| ğŸ”¥ğŸ”¥ğŸ”¥[Yao et al.] Sirllm: Streaming infinite retentive llm(@Yao)|[[pdf]](https://arxiv.org/pdf/2405.12528 ) | |â­ï¸â­ï¸â­ï¸ |
|2024| ğŸ”¥ğŸ”¥ğŸ”¥[Liu et al.] Think-in-memory: Recalling and post-thinking enable llms with long-term memory(@Liu)|[[pdf]](https://arxiv.org/pdf/2311.08719 ) | |â­ï¸â­ï¸â­ï¸ |
|2024| ğŸ”¥ğŸ”¥ğŸ”¥[Xu et al.] Think: Thinner key cache by query-driven pruning(@Xu)|[[pdf]](https://arxiv.org/pdf/2407.21018 ) | |â­ï¸â­ï¸â­ï¸ |
|2024| ğŸ”¥ğŸ”¥ğŸ”¥[Zhang et al.] H2o: Heavy-hitter oracle for efficient generative inference of large language models(@Zhang)|[[pdf]](https://arxiv.org/pdf/2406.07056 ) | |â­ï¸â­ï¸â­ï¸ |
|2024| ğŸ”¥ğŸ”¥ğŸ”¥[Yao et al.] Cacheblend: Fast large language model serving for rag with cached knowledge fusion(@Yao)|[[pdf]](https://arxiv.org/pdf/2405.16444 ) | |â­ï¸â­ï¸â­ï¸ |
|2024a| ğŸ”¥ğŸ”¥ğŸ”¥[Wang et al.] Recursively summarizing enables long-term dialogue memory in large language models(@Wang)|[[pdf]](https://arxiv.org/pdf/2308.15022 ) | |â­ï¸â­ï¸â­ï¸ |
|2024b| ğŸ”¥ğŸ”¥ğŸ”¥[Wang et al.] Squeezeattention: 2d management of kv-cache in llm inference via layer-wise optimal budget(@Wang)|[[pdf]](https://arxiv.org/pdf/2404.04793 ) | |â­ï¸â­ï¸â­ï¸ |
|2024| ğŸ”¥ğŸ”¥ğŸ”¥[Xu et al.] Think: Thinner key cache by query-driven pruning(@Xu)|[[pdf]](https://arxiv.org/pdf/2407.21018 ) | |â­ï¸â­ï¸â­ï¸ |
|2024| ğŸ”¥ğŸ”¥ğŸ”¥[Fu et al.] Moa: Mixture of sparse attention for automatic large language model compression(@Fu)|[[pdf]](https://arxiv.org/pdf/2406.14909 ) | |â­ï¸â­ï¸â­ï¸ |
|2024| ğŸ”¥ğŸ”¥ğŸ”¥[Song et al.] Zebra: Extending context window with layerwise grouped local-global attention(@Song)|[[pdf]](https://arxiv.org/pdf/2312.08618 ) | |â­ï¸â­ï¸â­ï¸ |
|2024| ğŸ”¥ğŸ”¥ğŸ”¥[Lu et al.] Longheads: Multi-head attention is secretly a long context processor(@Lu)|[[pdf]](https://arxiv.org/pdf/2402.10685 ) | |â­ï¸â­ï¸â­ï¸ |
|2024| ğŸ”¥ğŸ”¥ğŸ”¥[Li et al.] Snapkv: Llm knows what you are looking for before generation(@Li)|[[pdf]](https://arxiv.org/pdf/2404.18057 ) | |â­ï¸â­ï¸â­ï¸ |
|2024| ğŸ”¥ğŸ”¥ğŸ”¥[He et al.] Hmt: Hierarchical memory transformer for long context language processing(@He)|[[pdf]](https://arxiv.org/pdf/2405.02441 ) | |â­ï¸â­ï¸â­ï¸ |
|2024| ğŸ”¥ğŸ”¥ğŸ”¥[Hwang et al.] Transformerfam: Feedback attention is working memory(@Hwang)|[[pdf]](https://arxiv.org/pdf/2404.09173 ) | |â­ï¸â­ï¸â­ï¸ |
|2024| ğŸ”¥ğŸ”¥ğŸ”¥[Roy et al.] Efficient content-based sparse attention with routing transformers(@Roy)|[[pdf]](https://arxiv.org/pdf/2105.14788 ) | |â­ï¸â­ï¸â­ï¸ |
|2024| ğŸ”¥ğŸ”¥ğŸ”¥[Devoto et al.] A simple and effective l_2 norm-based strategy for kv cache compression(@Devoto)|[[pdf]](https://arxiv.org/pdf/2406.11430 ) | |â­ï¸â­ï¸â­ï¸ |
|2024| ğŸ”¥ğŸ”¥ğŸ”¥[Dong et al.] Get more with less: Synthesizing recurrence with kv cache compression for efficient llm inference(@Dong)|[[pdf]](https://arxiv.org/pdf/2402.09398 ) | |â­ï¸â­ï¸â­ï¸ |
|2024| ğŸ”¥ğŸ”¥ğŸ”¥[Feng et al.] Optimizing kv cache eviction in llms: Adaptive allocation for enhanced budget utilization(@Feng)|[[pdf]](https://arxiv.org/pdf/2407.20485 ) | |â­ï¸â­ï¸â­ï¸ |
|2024| ğŸ”¥ğŸ”¥ğŸ”¥[He and Zhai] Fastdecode: High-throughput gpu-efficient llm serving using heterogeneous pipelines(@He)|[[pdf]](https://arxiv.org/pdf/2403.11421 ) | |â­ï¸â­ï¸â­ï¸ |
|2024| ğŸ”¥ğŸ”¥ğŸ”¥[Pan et al.] Instinfer: In-storage attention offloading for cost-effective long-context llm inference(@Pan)|[[pdf]](https://arxiv.org/pdf/2409.04992 ) | |â­ï¸â­ï¸â­ï¸ |
|2024| ğŸ”¥ğŸ”¥ğŸ”¥[Nawrot et al.] Dynamic memory compression: Retrofitting llms for accelerated inference(@Nawrot)|[[pdf]](https://arxiv.org/pdf/2403.09636 ) | |â­ï¸â­ï¸â­ï¸ |
|2024| ğŸ”¥ğŸ”¥ğŸ”¥[Hu et al.] Memserve: Context caching for disaggregated llm serving with elastic memory pool(@Hu)|[[pdf]](https://arxiv.org/pdf/2406.17565 ) | |â­ï¸â­ï¸â­ï¸ |
|2024| ğŸ”¥ğŸ”¥ğŸ”¥[Prabhu et al.] Vattention: Dynamic memory management for serving llms without pagedattention(@Prabhu)|[[pdf]](https://arxiv.org/pdf/2405.04437 ) | |â­ï¸â­ï¸â­ï¸ |
|2023| ğŸ”¥ğŸ”¥ğŸ”¥[Kwon et al.] Efficient memory management for large language model serving with pagedattention(@Kwon)|[[pdf]](https://arxiv.org/pdf/2305.04473 ) | |â­ï¸â­ï¸â­ï¸ |
|2024| ğŸ”¥ğŸ”¥ğŸ”¥[Chen et al.] Magicdec: Breaking the latency-throughput tradeoff for long context generation with speculative decoding(@Chen)|[[pdf]](https://arxiv.org/pdf/2408.11049 ) | |â­ï¸â­ï¸â­ï¸ |
|2024| ğŸ”¥ğŸ”¥ğŸ”¥[Lin et al.] Infinite-llm: Efficient llm service for long context with distattention and distributed kvcache(@Lin)|[[pdf]](https://arxiv.org/pdf/2401.02669 ) | |â­ï¸â­ï¸â­ï¸ |
|2024| ğŸ”¥ğŸ”¥ğŸ”¥[He and Wu] Kcache: Efficient llm inference with kcache(@He)|[[pdf]](https://arxiv.org/pdf/2404.18057 ) | |â­ï¸â­ï¸â­ï¸ |
|2024| ğŸ”¥ğŸ”¥ğŸ”¥[Liao and Vargas] Beyond kv caching: Shared attention for efficient llms(@Liao)|[[pdf]](https://arxiv.org/pdf/2407.12866 ) | |â­ï¸â­ï¸â­ï¸ |
|2024| ğŸ”¥ğŸ”¥ğŸ”¥[Ataee Tarzanagh et al.] Max-margin token selection in attention mechanism(@Ataee)|[[pdf]](https://arxiv.org/pdf/2306.00057 ) | |â­ï¸â­ï¸â­ï¸ |


### Parameters-Based Memory
<div id="Parameters-Based-Memory"></div>

|Date|Title|Paper|Code|Recom|Comment|
|:---:|:---:|:---:|:---:|:---:|:---:|
|2023| ğŸ”¥ğŸ”¥ğŸ”¥[Kim et al.] Compressed context memory for online language model interaction(@Kim)|[[pdf]](https://arxiv.org/pdf/2312.03414 ) | |â­ï¸â­ï¸â­ï¸ |
|2020| ğŸ”¥ğŸ”¥ğŸ”¥[Sun et al.] Test-time training with self-supervision for generalization under distribution shifts(@Sun)|[[pdf]](https://arxiv.org/pdf/2004.08441 ) | |â­ï¸â­ï¸â­ï¸ |
|2022| ğŸ”¥ğŸ”¥ğŸ”¥[Gandelsman et al.] Test-time training with masked autoencoders(@Gandelsman)|[[pdf]](https://arxiv.org/pdf/2206.07730 ) | |â­ï¸â­ï¸â­ï¸ |
|2023| ğŸ”¥ğŸ”¥ğŸ”¥[Hardt and Sun] Test-time training on nearest neighbors for large language models(@Hardt)|[[pdf]](https://arxiv.org/pdf/2305.18466 ) | |â­ï¸â­ï¸â­ï¸ |
|2024| ğŸ”¥ğŸ”¥ğŸ”¥[Sukhbaatar et al.] Branch-train-mix: Mixing expert llms into a mixture-of-experts llm(@Sukhbaatar)|[[pdf]](https://arxiv.org/pdf/2403.07816 ) | |â­ï¸â­ï¸â­ï¸ |

### Hidden-State-Based Memory
<div id="Hidden-State-Based-Memory"></div>

|Date|Title|Paper|Code|Recom|Comment|
|:---:|:---:|:---:|:---:|:---:|:---:|
|2024| ğŸ”¥ğŸ”¥ğŸ”¥[Bai et al.] Citrus: Chunked instruction-aware state eviction for long sequence modeling(@Bai)|[[pdf]](https://arxiv.org/pdf/2406.12018 ) | |â­ï¸â­ï¸â­ï¸ |
|2024| ğŸ”¥ğŸ”¥ğŸ”¥[Willette et al.] Training-free exponential extension of sliding window context with cascading kv cache(@Willette)|[[pdf]](https://arxiv.org/pdf/2406.08454 ) | |â­ï¸â­ï¸â­ï¸ |
|2023| ğŸ”¥ğŸ”¥ğŸ”¥[Corallo and Papotti] Finch: Prompt-guided key-value cache compression for large language models(@Corallo)|[[pdf]](https://arxiv.org/pdf/2404.18057 ) | |â­ï¸â­ï¸â­ï¸ |
|2023| ğŸ”¥ğŸ”¥ğŸ”¥[Bertsch et al.] Unlimiformer: Long-range transformers with unlimited length input(@Bertsch)|[[pdf]](https://arxiv.org/pdf/2312.17259 ) | |â­ï¸â­ï¸â­ï¸ |
|2023| ğŸ”¥ğŸ”¥ğŸ”¥[Chevalier et al.] Adapting language models to compress contexts(@Chevalier)|[[pdf]](https://arxiv.org/pdf/2305.14788 ) | |â­ï¸â­ï¸â­ï¸ |
|2023| ğŸ”¥ğŸ”¥ğŸ”¥[Yu et al.] Mamba: A multiscale decoder architecture for long contexts(@Yu)|[[pdf]](https://arxiv.org/pdf/2312.08618 ) | |â­ï¸â­ï¸â­ï¸ |
|2023| ğŸ”¥ğŸ”¥ğŸ”¥[Xie et al.] Chunk, align, select: A simple long-sequence processing method for transformers(@Xie)|[[pdf]](https://arxiv.org/pdf/2308.13191 ) | |â­ï¸â­ï¸â­ï¸ |
|2024| ğŸ”¥ğŸ”¥ğŸ”¥[Ye et al.] Chunkattention: Efficient self-attention with prefix-aware kv cache and two-phase partition(@Ye)|[[pdf]](https://arxiv.org/pdf/2402.15220 ) | |â­ï¸â­ï¸â­ï¸ |
|2024| ğŸ”¥ğŸ”¥ğŸ”¥[An et al.] Training-free long-context scaling of large language models(@An)|[[pdf]](https://arxiv.org/pdf/2402.17463 ) | |â­ï¸â­ï¸â­ï¸ |
|2023| ğŸ”¥ğŸ”¥ğŸ”¥[Ivgi et al.] SLED: Sliding Encoder and Decoder(@Ivgi)|[[pdf]](https://arxiv.org/pdf/2311.18743 ) | |â­ï¸â­ï¸â­ï¸ |
|2023| ğŸ”¥ğŸ”¥ğŸ”¥[Peng et al.] Rwkv: Reinventing rnns for the transformer era(@Peng)|[[pdf]](https://arxiv.org/pdf/2305.13048 ) | |â­ï¸â­ï¸â­ï¸ |


## Â©ï¸License  

GNU General Public License v3.0  

## ğŸ‰Contribute  

Welcome to star & submit a PR to this repo! 



```BibTeX
@article{shan2025cognitive,
  title={Cognitive memory in large language models},
  author={Shan, Lianlei and Luo, Shixian and Zhu, Zezhou and Yuan, Yu and Wu, Yong},
  journal={arXiv preprint arXiv:2504.02441},
  year={2025}
}
```
