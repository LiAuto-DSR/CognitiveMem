
<!-- ![llm-kv-cache](https://github.com/Zefan-Cai/Awesome-LLM-KV-Cache/assets/31974251/4d9ab775-f200-471d-a289-e2b14296b633) -->

<!-- <div align='center'>
  <img src=https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg >
  <img src=https://img.shields.io/github/downloads/Zefan-Cai/Awesome-LLM-KV-Cache/total?color=ccf&label=downloads&logo=github&logoColor=lightgrey >
  <img src=https://img.shields.io/github/forks/Zefan-Cai/Awesome-LLM-KV-Cache.svg?style=social >
  <img src=https://img.shields.io/github/stars/Zefan-Cai/Awesome-LLM-KV-Cache.svg?style=social >
  <img src=https://img.shields.io/github/watchers/Zefan-Cai/Awesome-LLM-KV-Cache.svg?style=social >
  <img src=https://img.shields.io/badge/Release-v1.6-brightgreen.svg >
  <img src=https://img.shields.io/badge/License-GPLv3.0-turquoise.svg >
 </div>    -->

## 📒Introduction
Cognitive Memory in Large Language Models: A Curated List of [📙 Papers on Cognitive Memory Mechanisms in LLMs with Codes](#paperlist). This repository is for personal use of learning and classifying the latest research papers related to cognitive memory in large language models!

## ©️Citations 


## 📖Contents 
<div id="paperlist"></div>

* 📖[Text-based Memory](#Text-based-Memory)🔥🔥🔥
* 📖[KV Cache-Based Memory](#KV-Cache-Based-Memory)🔥🔥
* 📖[Parameters-Based Memory](#Parameters-Based-Memory)🔥🔥
* 📖[Hidden-State-Based Memory](#Hidden-State-Based-Memory)🔥


<!-- ### 📖Trending Inference Topics ([©️back👆🏻](#paperlist))  
<div id="Trending-Inference-Topics"></div>  

|Date|Title|Paper|Code|Recom|Comment|
|:---:|:---:|:---:|:---:|:---:|:---:| -->




<!-- ### Cognitive Architecture
|2020| 🔥🔥🔥[Kotseruba and Tsotsos] 40 years of cognitive architectures: core cognitive abilities and practical applications(@Kotseruba)|[[pdf]](https://arxiv.org/pdf/2004.08441 ) | |⭐️⭐️⭐️ |
|2019| 🔥🔥🔥[Laird] The Soar Cognitive Architecture(@Laird)|[[pdf]](https://mitpress.mit.edu/books/soar-cognitive-architecture ) | |⭐️⭐️⭐️ |
|2019| 🔥🔥🔥[Ritter et al.] The ACT-R Cognitive Architecture(@Ritter)|[[pdf]](https://act-r.psy.cmu.edu/) | |⭐️⭐️⭐️ |
|2006| 🔥🔥🔥[Langley and Choi] A unified cognitive architecture for physical agents(@Langley)|[[pdf]](https://www.aaai.org/Papers/AAAI/2006/AAAI06-1469.pdf ) | |⭐️⭐️⭐️ | -->

### Text-based Memory

<div id="Text-based-Memory"></div>

|Date|Title|Paper|Code|Recom|Comment|
|:---:|:---:|:---:|:---:|:---:|:---:|
|2024b| 🔥🔥🔥[Chen et al.] Compress to impress: Unleashing the potential of compressive memory in real-world long-term conversations(@Chen)|[[pdf]](https://arxiv.org/pdf/2402.11975 ) | |⭐️⭐️⭐️ |
|2024b| 🔥🔥🔥[Chen et al.] Compress to impress: Unleashing the potential of compressive memory in real-world long-term conversations(@Chen)|[[pdf]](https://arxiv.org/pdf/2402.11975 ) | |⭐️⭐️⭐️ |
|2024| 🔥🔥🔥[Zhong et al.] Memorybank: Enhancing large language models with long-term memory(@Zhong)|[[pdf]](https://arxiv.org/pdf/2404.02441 ) | |⭐️⭐️⭐️ |
|2024| 🔥🔥🔥[Lee et al.] A human-inspired reading agent with gist memory of very long contexts(@Lee)|[[pdf]](https://arxiv.org/pdf/2402.09727 ) | |⭐️⭐️⭐️ |
|2023| 🔥🔥🔥[Liu et al.] Think-in-memory: Recalling and post-thinking enable llms with long-term memory(@Liu)|[[pdf]](https://arxiv.org/pdf/2311.08719 ) | |⭐️⭐️⭐️ |
|2022| 🔥🔥🔥[Qi et al.] Training-free long-context scaling of large language models(@Qi)|[[pdf]](https://arxiv.org/pdf/2402.17463 ) | |⭐️⭐️⭐️ |
|2023| 🔥🔥🔥[Anagnostidis et al.] Dynamic context pruning for efficient and interpretable autoregressive transformers(@Anagnostidis)|[[pdf]](https://arxiv.org/pdf/2306.00057 ) | |⭐️⭐️⭐️ |
|2023| 🔥🔥🔥[Pagliardini et al.] Faster causal attention over large sequences through sparse flash attention(@Pagliardini)|[[pdf]](https://arxiv.org/pdf/2306.01160 ) | |⭐️⭐️⭐️ |
|2023| 🔥🔥🔥[Chevalier et al.] Adapting language models to compress contexts(@Chevalier)|[[pdf]](https://arxiv.org/pdf/2305.14788 ) | |⭐️⭐️⭐️ |
|2024| 🔥🔥🔥[Corallo and Papotti] Finch: Prompt-guided key-value cache compression for large language models(@Corallo)|[[pdf]](https://arxiv.org/pdf/2404.18057 ) | |⭐️⭐️⭐️ |
|2023| 🔥🔥🔥[Bertsch et al.] Unlimiformer: Long-range transformers with unlimited length input(@Bertsch)|[[pdf]](https://arxiv.org/pdf/2312.17259 ) | |⭐️⭐️⭐️ |
|2023| 🔥🔥🔥[Yu et al.] Mamba: A multiscale decoder architecture for long contexts(@Yu)|[[pdf]](https://arxiv.org/pdf/2312.08618 ) | |⭐️⭐️⭐️ |
|2023| 🔥🔥🔥[Xie et al.] Chunk, align, select: A simple long-sequence processing method for transformers(@Xie)|[[pdf]](https://arxiv.org/pdf/2308.13191 ) | |⭐️⭐️⭐️ |
|2024| 🔥🔥🔥[Ye et al.] Chunkattention: Efficient self-attention with prefix-aware kv cache and two-phase partition(@Ye)|[[pdf]](https://arxiv.org/pdf/2402.15220 ) | |⭐️⭐️⭐️ |
|2024| 🔥🔥🔥[An et al.] Training-free long-context scaling of large language models(@An)|[[pdf]](https://arxiv.org/pdf/2402.17463 ) | |⭐️⭐️⭐️ |
|2023| 🔥🔥🔥[Ivgi et al.] SLED: Sliding Encoder and Decoder(@Ivgi)|[[pdf]](https://arxiv.org/pdf/2311.18743 ) | |⭐️⭐️⭐️ |

### KV Cache-Based Memory
<div id="KV-Cache-Based-Memory"></div>

|Date|Title|Paper|Code|Recom|Comment|
|:---:|:---:|:---:|:---:|:---:|:---:|
|2024| 🔥🔥🔥[Wan et al.] D2o: Dynamic discriminative operations for efficient generative inference of large language models(@Wan)|[[pdf]](https://arxiv.org/pdf/2406.13035 ) | |⭐️⭐️⭐️ |
|2022| 🔥🔥🔥[Guo et al.] Longt5: Efficient text-to-text transformer for long sequences(@Guo)|[[pdf]](https://arxiv.org/pdf/2204.08441 ) | |⭐️⭐️⭐️ |
|2024b| 🔥🔥🔥[Liu et al.] Think-in-memory: Recalling and post-thinking enable llms with long-term memory(@Liu)|[[pdf]](https://arxiv.org/pdf/2311.08719 ) | |⭐️⭐️⭐️ |
|2024| 🔥🔥🔥[Dai et al.] Sequence can secretly tell you what to discard(@Dai)|[[pdf]](https://arxiv.org/pdf/2404.02222 ) | |⭐️⭐️⭐️ |
|2024| 🔥🔥🔥[Xiao et al.] Snapkv: Llm knows what you are looking for before generation(@Xiao)|[[pdf]](https://arxiv.org/pdf/2404.18057 ) | |⭐️⭐️⭐️ |
|2023| 🔥🔥🔥[Ding et al.] Longnet: Scaling transformers to 1,000,000,000 tokens(@Ding)|[[pdf]](https://arxiv.org/pdf/2307.02486 ) | |⭐️⭐️⭐️ |
|2020| 🔥🔥🔥[Beltagy et al.] Longformer: The long-document transformer(@Beltagy)|[[pdf]](https://arxiv.org/pdf/2004.05150 ) | |⭐️⭐️⭐️ |
|2020| 🔥🔥🔥[Ainslie et al.] Etc: Encoding long and structured inputs in transformers(@Ainslie)|[[pdf]](https://arxiv.org/pdf/2004.08441 ) | |⭐️⭐️⭐️ |
|2023| 🔥🔥🔥[Gao et al.] Empower your model with longer and better context comprehension(@Gao)|[[pdf]](https://arxiv.org/pdf/2307.13365 ) | |⭐️⭐️⭐️ |
|2024| 🔥🔥🔥[Jo and Shin] A2sf: Accumulative attention scoring with forgetting factor for token pruning in transformer decoder(@Jo)|[[pdf]](https://arxiv.org/pdf/2407.20485 ) | |⭐️⭐️⭐️ |
|2024b| 🔥🔥🔥[Tang et al.] Quest: Query-aware sparsity for efficient long-context llm inference(@Tang)|[[pdf]](https://arxiv.org/pdf/2406.10774 ) | |⭐️⭐️⭐️ |
|2024| 🔥🔥🔥[Zhang et al.] Efficient sparse attention needs adaptive token release(@Zhang)|[[pdf]](https://arxiv.org/pdf/2407.02328 ) | |⭐️⭐️⭐️ |
|2024| 🔥🔥🔥[Adnan et al.] Keyformer: Kv cache reduction through key tokens selection for efficient generative inference(@Adnan)|[[pdf]](https://arxiv.org/pdf/2404.12335 ) | |⭐️⭐️⭐️ |
|2024| 🔥🔥🔥[Zhao et al.] Alisa: Accelerating large language model inference via sparsity-aware kv caching(@Zhao)|[[pdf]](https://arxiv.org/pdf/2407.01143 ) | |⭐️⭐️⭐️ |
|2024| 🔥🔥🔥[Kim et al.] Compressed context memory for online language model interaction(@Kim)|[[pdf]](https://arxiv.org/pdf/2312.03414 ) | |⭐️⭐️⭐️ |
|2024| 🔥🔥🔥[Yao et al.] Sirllm: Streaming infinite retentive llm(@Yao)|[[pdf]](https://arxiv.org/pdf/2405.12528 ) | |⭐️⭐️⭐️ |
|2024| 🔥🔥🔥[Liu et al.] Think-in-memory: Recalling and post-thinking enable llms with long-term memory(@Liu)|[[pdf]](https://arxiv.org/pdf/2311.08719 ) | |⭐️⭐️⭐️ |
|2024| 🔥🔥🔥[Xu et al.] Think: Thinner key cache by query-driven pruning(@Xu)|[[pdf]](https://arxiv.org/pdf/2407.21018 ) | |⭐️⭐️⭐️ |
|2024| 🔥🔥🔥[Zhang et al.] H2o: Heavy-hitter oracle for efficient generative inference of large language models(@Zhang)|[[pdf]](https://arxiv.org/pdf/2406.07056 ) | |⭐️⭐️⭐️ |
|2024| 🔥🔥🔥[Yao et al.] Cacheblend: Fast large language model serving for rag with cached knowledge fusion(@Yao)|[[pdf]](https://arxiv.org/pdf/2405.16444 ) | |⭐️⭐️⭐️ |
|2024a| 🔥🔥🔥[Wang et al.] Recursively summarizing enables long-term dialogue memory in large language models(@Wang)|[[pdf]](https://arxiv.org/pdf/2308.15022 ) | |⭐️⭐️⭐️ |
|2024b| 🔥🔥🔥[Wang et al.] Squeezeattention: 2d management of kv-cache in llm inference via layer-wise optimal budget(@Wang)|[[pdf]](https://arxiv.org/pdf/2404.04793 ) | |⭐️⭐️⭐️ |
|2024| 🔥🔥🔥[Xu et al.] Think: Thinner key cache by query-driven pruning(@Xu)|[[pdf]](https://arxiv.org/pdf/2407.21018 ) | |⭐️⭐️⭐️ |
|2024| 🔥🔥🔥[Fu et al.] Moa: Mixture of sparse attention for automatic large language model compression(@Fu)|[[pdf]](https://arxiv.org/pdf/2406.14909 ) | |⭐️⭐️⭐️ |
|2024| 🔥🔥🔥[Song et al.] Zebra: Extending context window with layerwise grouped local-global attention(@Song)|[[pdf]](https://arxiv.org/pdf/2312.08618 ) | |⭐️⭐️⭐️ |
|2024| 🔥🔥🔥[Lu et al.] Longheads: Multi-head attention is secretly a long context processor(@Lu)|[[pdf]](https://arxiv.org/pdf/2402.10685 ) | |⭐️⭐️⭐️ |
|2024| 🔥🔥🔥[Li et al.] Snapkv: Llm knows what you are looking for before generation(@Li)|[[pdf]](https://arxiv.org/pdf/2404.18057 ) | |⭐️⭐️⭐️ |
|2024| 🔥🔥🔥[He et al.] Hmt: Hierarchical memory transformer for long context language processing(@He)|[[pdf]](https://arxiv.org/pdf/2405.02441 ) | |⭐️⭐️⭐️ |
|2024| 🔥🔥🔥[Hwang et al.] Transformerfam: Feedback attention is working memory(@Hwang)|[[pdf]](https://arxiv.org/pdf/2404.09173 ) | |⭐️⭐️⭐️ |
|2024| 🔥🔥🔥[Roy et al.] Efficient content-based sparse attention with routing transformers(@Roy)|[[pdf]](https://arxiv.org/pdf/2105.14788 ) | |⭐️⭐️⭐️ |
|2024| 🔥🔥🔥[Devoto et al.] A simple and effective l_2 norm-based strategy for kv cache compression(@Devoto)|[[pdf]](https://arxiv.org/pdf/2406.11430 ) | |⭐️⭐️⭐️ |
|2024| 🔥🔥🔥[Dong et al.] Get more with less: Synthesizing recurrence with kv cache compression for efficient llm inference(@Dong)|[[pdf]](https://arxiv.org/pdf/2402.09398 ) | |⭐️⭐️⭐️ |
|2024| 🔥🔥🔥[Feng et al.] Optimizing kv cache eviction in llms: Adaptive allocation for enhanced budget utilization(@Feng)|[[pdf]](https://arxiv.org/pdf/2407.20485 ) | |⭐️⭐️⭐️ |
|2024| 🔥🔥🔥[He and Zhai] Fastdecode: High-throughput gpu-efficient llm serving using heterogeneous pipelines(@He)|[[pdf]](https://arxiv.org/pdf/2403.11421 ) | |⭐️⭐️⭐️ |
|2024| 🔥🔥🔥[Pan et al.] Instinfer: In-storage attention offloading for cost-effective long-context llm inference(@Pan)|[[pdf]](https://arxiv.org/pdf/2409.04992 ) | |⭐️⭐️⭐️ |
|2024| 🔥🔥🔥[Nawrot et al.] Dynamic memory compression: Retrofitting llms for accelerated inference(@Nawrot)|[[pdf]](https://arxiv.org/pdf/2403.09636 ) | |⭐️⭐️⭐️ |
|2024| 🔥🔥🔥[Hu et al.] Memserve: Context caching for disaggregated llm serving with elastic memory pool(@Hu)|[[pdf]](https://arxiv.org/pdf/2406.17565 ) | |⭐️⭐️⭐️ |
|2024| 🔥🔥🔥[Prabhu et al.] Vattention: Dynamic memory management for serving llms without pagedattention(@Prabhu)|[[pdf]](https://arxiv.org/pdf/2405.04437 ) | |⭐️⭐️⭐️ |
|2023| 🔥🔥🔥[Kwon et al.] Efficient memory management for large language model serving with pagedattention(@Kwon)|[[pdf]](https://arxiv.org/pdf/2305.04473 ) | |⭐️⭐️⭐️ |
|2024| 🔥🔥🔥[Chen et al.] Magicdec: Breaking the latency-throughput tradeoff for long context generation with speculative decoding(@Chen)|[[pdf]](https://arxiv.org/pdf/2408.11049 ) | |⭐️⭐️⭐️ |
|2024| 🔥🔥🔥[Lin et al.] Infinite-llm: Efficient llm service for long context with distattention and distributed kvcache(@Lin)|[[pdf]](https://arxiv.org/pdf/2401.02669 ) | |⭐️⭐️⭐️ |
|2024| 🔥🔥🔥[He and Wu] Kcache: Efficient llm inference with kcache(@He)|[[pdf]](https://arxiv.org/pdf/2404.18057 ) | |⭐️⭐️⭐️ |
|2024| 🔥🔥🔥[Liao and Vargas] Beyond kv caching: Shared attention for efficient llms(@Liao)|[[pdf]](https://arxiv.org/pdf/2407.12866 ) | |⭐️⭐️⭐️ |
|2024| 🔥🔥🔥[Ataee Tarzanagh et al.] Max-margin token selection in attention mechanism(@Ataee)|[[pdf]](https://arxiv.org/pdf/2306.00057 ) | |⭐️⭐️⭐️ |


### Parameters-Based Memory
<div id="Parameters-Based-Memory"></div>

|Date|Title|Paper|Code|Recom|Comment|
|:---:|:---:|:---:|:---:|:---:|:---:|
|2023| 🔥🔥🔥[Kim et al.] Compressed context memory for online language model interaction(@Kim)|[[pdf]](https://arxiv.org/pdf/2312.03414 ) | |⭐️⭐️⭐️ |
|2020| 🔥🔥🔥[Sun et al.] Test-time training with self-supervision for generalization under distribution shifts(@Sun)|[[pdf]](https://arxiv.org/pdf/2004.08441 ) | |⭐️⭐️⭐️ |
|2022| 🔥🔥🔥[Gandelsman et al.] Test-time training with masked autoencoders(@Gandelsman)|[[pdf]](https://arxiv.org/pdf/2206.07730 ) | |⭐️⭐️⭐️ |
|2023| 🔥🔥🔥[Hardt and Sun] Test-time training on nearest neighbors for large language models(@Hardt)|[[pdf]](https://arxiv.org/pdf/2305.18466 ) | |⭐️⭐️⭐️ |
|2024| 🔥🔥🔥[Sukhbaatar et al.] Branch-train-mix: Mixing expert llms into a mixture-of-experts llm(@Sukhbaatar)|[[pdf]](https://arxiv.org/pdf/2403.07816 ) | |⭐️⭐️⭐️ |

### Hidden-State-Based Memory
<div id="Hidden-State-Based-Memory"></div>

|Date|Title|Paper|Code|Recom|Comment|
|:---:|:---:|:---:|:---:|:---:|:---:|
|2024| 🔥🔥🔥[Bai et al.] Citrus: Chunked instruction-aware state eviction for long sequence modeling(@Bai)|[[pdf]](https://arxiv.org/pdf/2406.12018 ) | |⭐️⭐️⭐️ |
|2024| 🔥🔥🔥[Willette et al.] Training-free exponential extension of sliding window context with cascading kv cache(@Willette)|[[pdf]](https://arxiv.org/pdf/2406.08454 ) | |⭐️⭐️⭐️ |
|2023| 🔥🔥🔥[Corallo and Papotti] Finch: Prompt-guided key-value cache compression for large language models(@Corallo)|[[pdf]](https://arxiv.org/pdf/2404.18057 ) | |⭐️⭐️⭐️ |
|2023| 🔥🔥🔥[Bertsch et al.] Unlimiformer: Long-range transformers with unlimited length input(@Bertsch)|[[pdf]](https://arxiv.org/pdf/2312.17259 ) | |⭐️⭐️⭐️ |
|2023| 🔥🔥🔥[Chevalier et al.] Adapting language models to compress contexts(@Chevalier)|[[pdf]](https://arxiv.org/pdf/2305.14788 ) | |⭐️⭐️⭐️ |
|2023| 🔥🔥🔥[Yu et al.] Mamba: A multiscale decoder architecture for long contexts(@Yu)|[[pdf]](https://arxiv.org/pdf/2312.08618 ) | |⭐️⭐️⭐️ |
|2023| 🔥🔥🔥[Xie et al.] Chunk, align, select: A simple long-sequence processing method for transformers(@Xie)|[[pdf]](https://arxiv.org/pdf/2308.13191 ) | |⭐️⭐️⭐️ |
|2024| 🔥🔥🔥[Ye et al.] Chunkattention: Efficient self-attention with prefix-aware kv cache and two-phase partition(@Ye)|[[pdf]](https://arxiv.org/pdf/2402.15220 ) | |⭐️⭐️⭐️ |
|2024| 🔥🔥🔥[An et al.] Training-free long-context scaling of large language models(@An)|[[pdf]](https://arxiv.org/pdf/2402.17463 ) | |⭐️⭐️⭐️ |
|2023| 🔥🔥🔥[Ivgi et al.] SLED: Sliding Encoder and Decoder(@Ivgi)|[[pdf]](https://arxiv.org/pdf/2311.18743 ) | |⭐️⭐️⭐️ |
|2023| 🔥🔥🔥[Peng et al.] Rwkv: Reinventing rnns for the transformer era(@Peng)|[[pdf]](https://arxiv.org/pdf/2305.13048 ) | |⭐️⭐️⭐️ |


## ©️License  

GNU General Public License v3.0  

## 🎉Contribute  

Welcome to star & submit a PR to this repo! 



```BibTeX
@article{shan2025cognitive,
  title={Cognitive memory in large language models},
  author={Shan, Lianlei and Luo, Shixian and Zhu, Zezhou and Yuan, Yu and Wu, Yong},
  journal={arXiv preprint arXiv:2504.02441},
  year={2025}
}
```
